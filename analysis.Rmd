---
output:
  bookdown::pdf_document2:
    includes:
      in_header: latex/preamble.tex
      before_body: latex/titlepage.tex
    pandoc_args:
    - --csl
    - references/apa.csl
  bookdown::html_document2:
    pandoc_args:
    - --csl
    - references/apa.csl
  bookdown::word_document2:
    pandoc_args:
    - --csl
    - references/apa.csl
toc-title: Table of Contents
bibliography: references/references.bib
link-citations: yes
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \renewcommand{\headrulewidth}{0pt}
- \fancyfoot[C]{}
- \fancyfoot[R]{\thepage}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, messages = FALSE)
packages <- c("dplyr","readxl", "curl", "ggplot2", "ggrepel", "maps", "plotly", "stringr", "tm", "wordcloud2", "tidyverse", "RColorBrewer", "ggwordcloud", "viridis", "bookdown", "utils", "leaps", "broom","GGally")
package.check <- lapply(packages, FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
        install.packages(x, dependencies = TRUE)
        library(x, character.only = TRUE)
    }
})
```
\fancyhead[LR]{}
\pagenumbering{roman}

\newpage
\cleardoublepage
\pagenumbering{arabic}
\fancyhead[L]{Vegan and Vegetarian Restaurant Model}
\fancyhead[R]{R-Bootcamp}
# Introduction

Inspired by Veganuary, we discovered a dataset about vegetarian and vegan restaurants in the US [@datafiniti2018]. The dataset was found through a dataset search on Google, hosted on Kaggel. The reason for selecting this dataset was that many useful parameters were already available. The description of the dataset states that it contains data from 18,0000 restaurants. Unfortunately, we only later noticed that only 211 individual restaurants with 10,000 menu items are included in the dataset, as it serves as an advertisement for the Datafiniti Business Database. The entire dataset is only available for purchase, and after consulting with the lecturer (Matteo Tanadini), it was agreed that the study should continue with less meaningful results.

The purpose of this paper is to examine the relationship between the number of vegetarian or vegan restaurants, population and median household income for the U.S. zip code [@mpsc2020], and the frequency of Google searches for vegetarian and vegan restaurants in that state [@vegan_google_trends; @vegetarian_google_trends]. With the small sample size of 211 restaurants and 10000 menu items, this study cannot significantly predict the number of vegetarian restaurants in a zip code.
Nevertheless, this paper offers some insights into vegetarian and vegan supply in the U.S., as well as the demand and whether demographics, income, and Google searches match up.

For the following work, different R Packages have been used. The respective packages can be found in the original rmarkdown file. 



# Methodology

In a first step relevant data has been researched. The data has been imported to R and first visualizations were made to get some insights into the data. The programming language R is used for the calculations in the following documents. For the sake simplicity not all codes are included. The whole code can be found in the original rmarkdown file. 

In the following document different calculations and plotting methods are applied. The different methods should reflect the teaching content from the R-Bootcamp as well as knowledge the Authors has been achived by themselves during the learning process. 


**Model**


$$ \text{restaurants} = {\beta}_0 + {\beta}_1 * \text{residents} + {\beta}_2 * \text{income} + {\beta}_3 * \text{searches} $$

The formula above shows the concept of the model. The model aims to explain the offers of vegetarian and vegan restaurants in the U.S. based on the demand in the different regions. The demand is interpret by the amount of searches on "vegan restaurants" and "vegetarian restaurants". Furthermore the model aims to explain a possible correlation between the offering and the demand in regards to the mean household income. 

## Importing Data

The following work is based on three different data sources. We are gathering data from [@datafiniti2018] through Kaggel (Restaurant Data), [@mpsc2020] (Mean household income U.S.) and from Google Trend [@vegan_google_trends; @vegetarian_google_trends] (Google Search Data).


```{r importing, warning=FALSE}
#setwd("/Users/larissaeisele/switchdrive/R-Bootcamp/project-R_Bootcamp")
restaurants <-read.csv("./Data/veg_restaurants_US.csv")
income_pop_by_zip <- read_excel("./Data/MeanZIP-3.xlsx")
google_searches_vegetarian <- read_excel("./Data/searches_vegetarian_res.xlsx")
google_searches_vegan <- read_excel("./Data/searches_vegan_res.xlsx")
```

\newpage

## Data Processing

### Restaurant Data

As a first step, we took a closer look into the restaurant data and decided which parameter was needed. With the following code, we're selecting the relevant parameter. Further, we had to change the type of zip code to numeric.

```{r restaurant data, warning=FALSE}
restaurants <- restaurants %>% select(id, dateAdded, cuisines, latitude, 
                                      longitude, menus.category, name, 
                                      province, postalCode)
restaurants$postalCode <- as.numeric(restaurants$postalCode)
restaurants <- restaurants %>% mutate(cuisines_split = strsplit(cuisines,","))
```

While looking closer into the restaurant data, we found a restaurant without a zip code. We looked up the respective restaurant and found the right zip code for the restaurant. We cleaned the missing data from the restaurant and added the zip code to the dataset with the following code.

```{r restaurant missing data fix, warning=FALSE}
restaurants[restaurants$id == "AV1Tp59p3D1zeR_xE2DL",]$postalCode <- 94596
anyNA(restaurants$postalCode) # There are no missing values anymore in postalCode (zip code)
```


### Population Size and Mean Household Income Data

To enrich our restaurant data, we joined the mean house income by zip code data. We did a left join with the join key of the zip codes of the two datasets.


**Joining the Data**
```{r adding household mean and pop size to dataset, warning=FALSE}
joined_data <- left_join(restaurants, income_pop_by_zip %>% 
                           select(Zip, Mean, Pop), by = c("postalCode" = "Zip"))
```

**Checking for missing data**
```{r missing data check, warning=FALSE}
sum(is.na(joined_data$Mean)) # all postal codes have a mean household income
sum(is.na(joined_data$Pop)) # all postal codes have a population
```

### Google Search Data

During the project, it occurred to us that it would be interesting to compare the demand with the supply of vegetarian and vegan restaurants. However, no data on vegetarian or vegan diets could be found from the U.S. population. Therefore, we decided to show the demand using Google search data. Therefore, we used the search query for "vegetarian restaurant" and "vegan restaurant" via the website of [https://trends.google.de/]. The last 5 years were chosen as the time period. 



```{r google search data, warning=FALSE}
# Vegetarian Restaurant Search Results
vegetarian_temp <- google_searches_vegetarian %>% group_by(province) %>% 
  summarise(total_searches_vegetarian = sum(search_vegetarian_res, na.rm = TRUE))
joined_data <- left_join(joined_data, vegetarian_temp %>% 
                           select(province, total_searches_vegetarian), by = "province")
# Vegan Restaurant Search Results
vegan_temp <- google_searches_vegan %>% group_by(province) %>% 
  summarise(total_searches_vegan = sum(search_vegan_res, na.rm = TRUE))
joined_data <- left_join(joined_data, vegan_temp %>% 
                           select(province, total_searches_vegan), by = "province")
```


## Missing Values

We checked the search data for missing values. Unfortunately, data is not available for all provinces in the U.S. No information was found from Google if the data is not collected for certain provinces or whether there are no searches for these provinces We consider the latter unlikely based on the selected time period. With the following code we grouped the search data by province. 

```{r missing data check google search results, warning=FALSE}
sum(is.na(joined_data$total_searches_vegetarian))
sum(is.na(joined_data$total_searches_vegan))
# 42 provinces have no google search hits
```


# Data Visualization

## Summary Statistics

In a first step we took a closer look into our data sources. With th following summary statistics on the mean income and population by Zip code [@mpsc2020] various insights can be gained. Regarding the Mean income, the following table shows the data is not cleaned as we have 7 missing values. However, the missing values do not influence our further analysis, therefore we decided to proceed with them. With regards to the insights, the table shows that the the difference between the income is very high. We can imagine that this is related to different living situations. For example, the data set may also include students with very low incomes, as well as households with two high earners. The summary statistic on the Population by Zipcode shows that in one Zipcode it seems that only one person is registered. We are aware that this is most probably not realistic but anyway we decided to use the data set for our analysis. Another interesting insight is that in the U.S. population by Zipcode has a Mean Population about 9193 which is equivalent to a small town in Switzerland.



```{r}
# yearly mean household income whole U.S.
summary(income_pop_by_zip %>% select(Mean, Pop))
```


```{r}
#summarize household income in U.S. 
income_pop_by_zip %>% group_by(Zip) %>% 
  summarize(mean=mean(Mean,na.rm=TRUE), std_dev =sd(Mean,na.rm=TRUE))
```

## Histogram

### Vegetarian and Vegan Restaurant by State

In the following Histograms the amount of vegetarian and vegan restaurants are shown by state, to get some insights how the data is distributed by the region. It can be seen that in our dataset, New York has the most records on vegetarian and vegan restaurants.California is the state with the second most vegetarian and vegan restaurants in our dataset. It can be seen that in our dataset the offering of vegetarian and vegan offers are spread quite equally beside New York and California.

```{r}
#Vegetarian and Vegan Restaurants by State
number_of_res <- joined_data %>% group_by(province) %>% 
  summarise(Number_Of_Restaurants = n_distinct(id))
ggplot(number_of_res, aes(x=province, y=Number_Of_Restaurants, fill=province)) + 
  geom_bar(stat = "identity") +
  ggtitle("Number of Restaurants by State") + 
  xlab("State") + ylab("Number of Restaurants") + labs(fill='State') +
  theme_minimal()
```
\newpage

### Mean Houshold Income by State

In the following histogram the mean household income is shown by state. Based on our dataset New York is the State with the highest mean household income, followed by Massachusetts and New Hampshire. The other states have a quite balanced mean household income. Nevada and Georgia have the lowest mean household income.


```{r}
# Mean Household income by State
temp <- joined_data %>% distinct(id, province, Mean) %>% group_by(province) %>% 
  summarise(Avg_Mean_Province = (sum(Mean) / n_distinct(Mean))) 
ggplot(data=temp, aes(x=province, y=Avg_Mean_Province, fill=province)) +
  geom_bar(stat="identity") +
  ggtitle("Yearly Mean Household Income by State") + 
  xlab("State") + ylab("Average Mean Household Income") + labs(fill='State') +
  theme_minimal()

```
\newpage

### Population by State

The following histogram shows the population per state. New York has the highest population, this is probably due to the high population density in the city of New York. The State Nevada has by far the lowest population density which can be explained with the environment and climate conditions in this region. 


```{r}
# Population by State
temp <- joined_data %>% distinct(id, province, Pop) %>% group_by(province) %>% 
  summarise(Avg_Pop = (sum(Pop) / n_distinct(Pop))) 
ggplot(data=temp, aes(x=province, y=Avg_Pop, fill=province)) +
  geom_bar(stat="identity") +
  ggtitle("Population by State") + 
  xlab("State") + ylab("Average Population") + labs(fill='State') +
  theme_minimal()
```

### Vegetarian and Vegan Searches by State

Based on the google search data Florida has the highest demand for vegetarian and vegan restaurants. The data does not fit with our findings from the restaurant data, based on this in Florida the offers for vegetarian and vegan restaurant ist not that high. 

```{r}
#summarize vegetarian & vegan searches
google_search_frequency <- joined_data %>% group_by(province) %>% 
  distinct(province, total_searches_vegan, total_searches_vegetarian) %>% 
  summarise(google_search_frequency = total_searches_vegan + total_searches_vegetarian)

ggplot(google_search_frequency, 
       aes(x=province, y=google_search_frequency, fill=province)) + 
  geom_bar(stat = "identity") +
  ggtitle("Google Searches by State") + 
  xlab("State") + ylab("Number of Google Searches") + labs(fill='State') +
  theme_minimal()
```

## Boxplots

### Household Income

With a Boxplot the distribution of at least one ordinal scaled value can be graphically shown. The following Boxplot shows the mean household income by state. The Median for Massachusetts seems to be quite high. In California and New Hampshire are some outliers with a very high mean household income. There are a few group of states which have almost the same median. 

```{r}
#Boxplot Household Income
ggplot(data=joined_data, aes(x=province, y=Mean, color=province)) +
  geom_boxplot(alpha=1) +
  ggtitle("Yearly Mean Household Income by State") + xlab("State") + ylab("Mean Household Income") + labs(fill='State') +
  theme_minimal()
```
### Population

The following shows the distribution of the population by state.

```{r}
#Boxplot Population
ggplot(data=joined_data, aes(x=province, y=Pop, color=province)) +
  geom_boxplot(alpha=1) +
  ggtitle("Population") + xlab("State") + ylab("Population") + labs(fill='State') +
  theme_minimal()
```

## Scatterplot

With scatterplots a relationship between two variables can be visualized. The following scatterplot shows the relationship between the demand of vegan and vegetarian restaurants and the amount of offers by region. 

```{r}
number_of_res <- joined_data %>% group_by(province) %>% 
  summarise(Number_Of_Restaurants_In_Province = n_distinct(id))
unique_res_data <- joined_data %>% distinct(id, 
                                            Pop, 
                                            Mean, 
                                            total_searches_vegetarian, 
                                            total_searches_vegan, province)
unique_res_data <- left_join(unique_res_data, number_of_res %>% 
                           select(province, Number_Of_Restaurants_In_Province), 
                           by = "province"
                           )
colnames(unique_res_data) <- c("ID", 
                               "State", 
                               "Mean Household Income at Zip Code", 
                               "Population Size at Zip Code", 
                               "Google Search Frequency for Vegetarian Restaurant", 
                               "Google Search Frequency for Vegan Restaurant", 
                               "Number of Restaurants in State")
```


```{r, echo=FALSE}
temp <- unique_res_data %>% select(`Number of Restaurants in State`, `Population Size at Zip Code`, `Mean Household Income at Zip Code`, `Google Search Frequency for Vegetarian Restaurant`, `Google Search Frequency for Vegan Restaurant`)

ggpairs(temp, aes(alpha = 0.4))
``` 

# Modeling
```{r}
fit <- lm(`Number of Restaurants in State` ~ `Population Size at Zip Code` + 
            `Mean Household Income at Zip Code` + 
            `Google Search Frequency for Vegetarian Restaurant` + 
            `Google Search Frequency for Vegan Restaurant`, 
          data=unique_res_data)
summary(fit)
```
## Model

\[
\begin{aligned}
\text{Number of Restaurants in a State} &= -34.59 \\
 & + 0.0007341 * \text{Population} \\
 & + 0.0001898 * \text{Mean Household Income} \\
 & + 0.9301 * \text{Google Search Frequency for Vegetarian Restaurant} \\
 & - 0.05598 * \text{Google Search Frequency for Vegan Restaurant}
\end{aligned}
\]

\newpage
## Interpretation

- The R-squared seems to indicate that ~56% of the variability of the data around its mean is explained by the model.
  The F-statistics p-value is below the significance level of 0.05% and thus significant, hence we reject the null hypothesis. 
  There is at least one $\beta$ significantly different from 0. Thus at least one variable contributes significantly to the model.
  Moreover based on the p-value's $\beta_{1}$, $\beta_{2}$, $\beta_{3}$ and $\beta_{4}$ all seem to significantly contribute to the model.
- With no population,  no income, and no Google searches there are negative ~35 restaurants in a state. 
- For each additional resident in U.S. state the amount of vegetarian/vegan restaurants increases by 0.0007341.
  Therefore, there is approximately one restaurant for every 1'400 residents at a specific zip code in a given state.
- For each $ in Mean Household Income the number of restaurants increases by 0.0001898.
  Thus, for approximately $5300 in Mean Household Income the number of restaurants increases by one for a given state.
- For every Google search with the term 'Vegetarian Restaurant' the number of restaurants increases by 0.9301.
- For every Google search with the term 'Vegan Restaurant' the number of restaurants decreases by 0.05598.


\newpage


## Most significant predictors
```{r, fig.show="hold", out.width="50%", warning=FALSE, echo=TRUE}
reg_test <- unique_res_data %>% select(`Number of Restaurants in State`, 
                                       `Population Size at Zip Code`,
                                       `Mean Household Income at Zip Code`,
                                       `Google Search Frequency for Vegetarian Restaurant`,
                                       `Google Search Frequency for Vegan Restaurant`)
reg <- regsubsets(`Number of Restaurants in State` ~ ., 
                  data = reg_test, 
                  method = "forward", 
                  nvmax = 6)
summary(reg)$which
```

Based on forward algorithm the predictors **population and mean household income** are most often included in the model (i.e most significant).

## Residuals

```{r, fig.show="hold", out.width="50%", warning=TRUE, echo=FALSE}
par(mar = c(4, 4, .1, .1))
model.diag.metrics <- augment(fit)
# Population
ggplot(model.diag.metrics, aes(`Number of Restaurants in State`, `Population Size at Zip Code`)) +
  geom_point() +
  stat_smooth(method = lm, se = FALSE) +
  geom_segment(aes(xend = `Number of Restaurants in State`, yend = .fitted), color = "red", size = 0.3)
# Income
ggplot(model.diag.metrics, aes(`Number of Restaurants in State`, `Mean Household Income at Zip Code`)) +
  geom_point() +
  stat_smooth(method = lm, se = FALSE) +
  geom_segment(aes(xend = `Number of Restaurants in State`, yend = .fitted), color = "red", size = 0.3)
# Google Search Frequency for Vegetarian Restaurant
ggplot(model.diag.metrics, aes(`Number of Restaurants in State`, `Google Search Frequency for Vegetarian Restaurant`)) +
  geom_point() +
  stat_smooth(method = lm, se = FALSE) +
  geom_segment(aes(xend = `Number of Restaurants in State`, yend = .fitted), color = "red", size = 0.3)
# Google Search Frequency for Vegan Restaurant
ggplot(model.diag.metrics, aes(`Number of Restaurants in State`, `Google Search Frequency for Vegan Restaurant`)) +
  geom_point() +
  stat_smooth(method = lm, se = FALSE) +
  geom_segment(aes(xend = `Number of Restaurants in State`, yend = .fitted), color = "red", size = 0.3)
```
There does not appear to be a clear pattern in the residual plots, however NY at 110 and CA at 33 are clearly outliers in this data set.
This is most likely due to the fact that the data set contains only a portion of the full data set.

\newpage
# Chapter of Choice: Maps, Wordcloud and Bookdown

## Maps and Wordcloud 

Since we were investigating location data, the idea of visualizing a map was quickly given. With the following code three different Maps has been visualized, to show the relation between the population size, the amount of restaurants and the google search frequency. 

#### Population by numbers of restaurants

Map one shows the population size referring to the amount of restaurants. It can be clearly seen that in New York and California the population is quite high and as well there can be found the most vegetarian and vegan restaurants offers. While in the State New York the highest population size is counted in the city of New York, in California there are more than one high population spread through the whole state. In a further analysis it could be interesting to see, where the most restaurants in the respective states are places and if it correlates with the population size. 

#### Mean household income by numbers of restaurants

Map two "Income", shows the mean household income referring to the amount of restaurants. I can be seen, that based on our dataset there is not much correlation between the amount of restaurants and the mean household income.

#### Google search frequency by numbers of restaurants

Map three "Google Search Frequency" shows the demand for vegetarian and vegan restaurants. Based on our dataset it can be interpreted that in Florida and California the demand is quite high, followed by Texas and New York. Ignoring the low quality from the data, it could be recommended to restaurants owner to open a vegetarian or vegan restaurants in the mention states. 


```{r, fig.show="hold", out.width="50%", warning=FALSE, echo=TRUE}

temp <- read.csv(
  curl("https://raw.githubusercontent.com/cphalpert/census-regions/master/us%20census%20bureau%20regions%20and%20divisions.csv")
)
states_map <- map_data("state")
states_map <- left_join(states_map, temp %>% mutate(State = tolower(State)) %>% 
                          select(State, State.Code), by = c("region" = "State"))
google_search_frequency <- joined_data %>% group_by(province) %>% 
  distinct(province, total_searches_vegan, total_searches_vegetarian) %>% 
  summarise(google_search_frequency = total_searches_vegan + total_searches_vegetarian)

# Map Data
states_map_google_search_frequency <- left_join(states_map, google_search_frequency, 
  by = c("State.Code" = "province"))
states_map_number_restaurants_state <- left_join(states_map, joined_data %>% 
  group_by(province) %>% 
  summarise(Number_Of_Restaurants = n_distinct(id)), by = c("State.Code" = "province"))
joined_data$MeanNormalized <- (joined_data$Mean - min(joined_data$Mean)) / 
  (max(joined_data$Mean) - min(joined_data$Mean))

# Population
ggplot() +
  # Number of Restaurants by state
  geom_polygon(data = states_map_number_restaurants_state, 
               aes(long, lat, group = group, 
                   fill=Number_Of_Restaurants), 
                   color = "white") + 
  scale_fill_gradientn("Number of Restaurants in State", 
                       colors = c('lightgreen', 'darkgreen'), 
                       breaks = seq(from = 0, to = 100, by = 15)) +
  
  # mapping 44 largest cities in the U.S. since 45 is in hawaii
  geom_point(data=us.cities %>% arrange(pop) %>% tail(44), 
             aes(x=long, y=lat, size = pop/1000000), 
                 color = "blue", alpha = 0.4) +
  labs(size="Largest U.S. Cities Population") + 
       scale_size(breaks=c(1, 3, 5, 8), 
                  labels=c("1 million","3 million","5 million","8 million"), 
                  guide="legend") +
  
  # mapping the veg. restaurants excluding hawaii (96753)
  geom_point(data=joined_data %>% filter(!str_detect(postalCode, "96753")), 
                                         aes(x=longitude, y=latitude), 
                                         size=0.3) +
  theme_void() + coord_map() + 
  theme(plot.title = element_text(face = "bold"), 
        legend.position="bottom", 
        legend.box="vertical", 
        legend.margin=margin(), 
        legend.box.just = "left") +
  labs(title = "Population")

# Income
ggplot() +
  # Number of Restaurants by state
  geom_polygon(data = states_map_number_restaurants_state, 
               aes(long, lat, group = group, fill=Number_Of_Restaurants), 
               color = "white") + 
  scale_fill_gradientn("Number of Restaurants in State         ", 
                       colors = c('lightgreen', 'darkgreen'), 
                       breaks = seq(from = 0, to = 100, by = 15)) +
  
  # mapping the veg. restaurants excluding hawaii (96753)
  geom_point(data=joined_data %>% filter(!str_detect(postalCode, "96753")), 
                                         aes(x=longitude, y=latitude, 
                                             color = MeanNormalized), 
                                         size=0.5, alpha = 0.4) +
  scale_color_gradientn("Normalized Mean Household Income", 
                        colors = c('red', 'darkorange', 'yellow', 'green'), 
                        breaks = seq(from = 0, to = 1, by = 0.25)) +
  theme_void() + coord_map() + 
  theme(plot.title = element_text(face = "bold"), 
        legend.position="bottom", 
        legend.box="vertical", 
        legend.margin=margin(), 
        legend.box.just = "left") +
  labs(title = "Income") + 
  guides(fill = guide_colourbar(order = 1))

par(mar = c(0, 5, 0, 0))
# Google Search Frequency
ggplot() +
  # Number of Google Searches by State
  geom_polygon(data = states_map_google_search_frequency, 
               aes(long, lat, group = group, fill=google_search_frequency), 
               color = "white") + 
  scale_fill_gradientn("Google Search Frequency in State", 
                       colors = c('lightblue', 'darkblue'), 
                       breaks = seq(from = 0, to = 5000, by = 1000)) +
  
  # mapping the veg. restaurants excluding hawaii (96753)
  geom_point(data=joined_data %>% filter(!str_detect(postalCode, "96753")), 
                                         aes(x=longitude, y=latitude), 
                                         size=0.5, 
                                         alpha = 0.4, 
                                         color = "black") +
  theme_void() + coord_map() + 
  theme(plot.title = element_text(face = "bold"), 
        legend.position="bottom", 
        legend.box="vertical", 
        legend.margin=margin(0,125), 
        legend.box.just = "left") +
  labs(title = "Google Search Frequency")

```


```{r, eval=knitr::is_html_output(), echo=knitr::is_html_output() }
knitr::asis_output("## Vegetarian and Vegan Cuisines (with wordcloud2)")
```

The following wordcloud is only visible with the html output. Wordcloud can be used to visual arrange words based on their frequency. With wordcloud2 we could visualize an interactive wordcloud with the most frequent cuisines type mention in our restaurant dataset. Indian seems to be the most offered cuisine type with regard to vegetarian and vegan offers. 

```{r, eval=knitr::is_html_output(), warning=FALSE, fig.width=5, fig.height=5, echo=knitr::is_html_output()}
# devtools::install_github("lchiffon/wordcloud2") fixed rmarkdown
df <- as.data.frame(do.call(rbind, 
                            lapply(joined_data$cuisines_split, as.data.frame)))
colnames(df) <- c("cuisines")

# remove leading whitespaces
df$cuisines <- trimws(df$cuisines)

# additional fixes
df$cuisines[df$cuisines == 'Dim Sum'] <- 'DimSum'
df$cuisines[df$cuisines == 'Hot Dogs'] <- 'HotDogs'
df$cuisines[df$cuisines == 'Fast Food'] <- 'FastFood'
df$cuisines[df$cuisines == 'Deli Food'] <- 'Deli'
df$cuisines[df$cuisines == 'Ice Cream and Desserts'] <- 'IceCream and Desserts'

# remove nonsense words
df <- df %>% filter(!(str_detect(cuisines, "Restaurant|Friendly|Vegetarian|Vegan|Health|Bar|Gluten|Bistro|Take|Local|Organic|Steak|Cater|North|South|Middle")))

# seperate words by slash; the word ' and '; whitespace
filtered <- df %>% separate_rows(cuisines, sep = "/") %>% 
  separate_rows(cuisines, sep = " and ") %>% 
  separate_rows(cuisines, sep = " ")
filtered <- filtered %>% 
  filter(!(str_detect(cuisines, "New|Modern|Small|Shops|Lunch|Pacific|American")))

v <- sort(rowSums(
  as.matrix(TermDocumentMatrix(
    Corpus((VectorSource(filtered$cuisines))))
         )),decreasing=TRUE)
word_frequency_df <- data.frame(word = names(v),freq=v)

wordcloud_colors <- colorRampPalette(brewer.pal(11,"BrBG"))
wordcloud2(word_frequency_df, size = 1, minRotation = pi/6, maxRotation = pi/6, 
           rotateRatio = 1, color=rev(wordcloud_colors(32)[seq(1,32,1)]))
```

```{r, eval=knitr::is_latex_output(), echo=knitr::is_latex_output() }
knitr::asis_output("## Vegetarian and Vegan Cuisines (with ggwordcloud)")
```

With ggwordcloud the wordcloud can also be exported as pdf output thus less interactive.

```{r, eval=knitr::is_latex_output(), warning=FALSE, fig.width=5, fig.height=5, echo=knitr::is_latex_output()}
# devtools::install_github("lchiffon/wordcloud2") fixed rmarkdown
df <- as.data.frame(do.call(rbind, 
                            lapply(joined_data$cuisines_split, as.data.frame)))
colnames(df) <- c("cuisines")

# remove leading whitespaces
df$cuisines <- trimws(df$cuisines)

# additional fixes
df$cuisines[df$cuisines == 'Dim Sum'] <- 'DimSum'
df$cuisines[df$cuisines == 'Hot Dogs'] <- 'HotDogs'
df$cuisines[df$cuisines == 'Fast Food'] <- 'FastFood'
df$cuisines[df$cuisines == 'Deli Food'] <- 'Deli'
df$cuisines[df$cuisines == 'Ice Cream and Desserts'] <- 'IceCream and Desserts'

# remove nonsense words
df <- df %>% filter(!(str_detect(cuisines, "Restaurant|Friendly|Vegetarian|Vegan|Health|Bar|Gluten|Bistro|Take|Local|Organic|Steak|Cater|North|South|Middle")))

# seperate words by slash; the word ' and '; whitespace
filtered <- df %>% separate_rows(cuisines, sep = "/") %>% 
  separate_rows(cuisines, sep = " and ") %>% 
  separate_rows(cuisines, sep = " ")
filtered <- filtered %>% 
  filter(!(str_detect(cuisines, "New|Modern|Small|Shops|Lunch|Pacific|American")))

v <- sort(rowSums(
  as.matrix(TermDocumentMatrix(
    Corpus((VectorSource(filtered$cuisines))))
         )),decreasing=TRUE)
word_frequency_df <- data.frame(word = names(v),freq=v)

ggplot(word_frequency_df, aes(label = word, size = freq, 
                              color = freq, angle = 30)) +
  geom_text_wordcloud(shape = "square", rm_outside = TRUE, max_steps = 1, 
                      grid_size = .1, eccentricity = .1) +
  scale_size_area(max_size = 10) +
  scale_color_viridis(option = "mako", direction = -1) + theme_void()
```

## Bookdown

This paper was optimized using the Bookdown package:

- a custom title page done in Latex
- a table of contents with roman lettering
- custom header and footer notations
- references in APA7 using BibTex
- optimized exports for HTML (interactive) and PDF (static)

\newpage

# Reflection and Conclusion on the Project

Data Quality / Amount of Data 

# Conclusion for ourselves 

The R-Bootcamp Week was for both of us very valuable and the content of the course covered various important material for our further studies as well as working life.The main learning process for us was achieved through the final project. It was very instructive to work independently on a project and to deepen and expand the basics we had learned. We are both positively surprised by our learning curve.


\newpage
# References