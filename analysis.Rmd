---
output:
  bookdown::pdf_document2:
    includes:
      in_header: preamble.tex
      before_body: titlepage.tex
    pandoc_args:
    - --csl
    - apa.csl
  bookdown::html_document2:
    pandoc_args:
    - --csl
    - apa.csl
  bookdown::word_document2:
    pandoc_args:
    - --csl
    - apa.csl
toc-title: "Table of Contents"
bibliography: references.bib
link-citations: yes
header-includes:
  - \usepackage{fancyhdr}
  - \pagestyle{fancy}
  - \fancyhead[L]{}
  - \fancyhead[L]{Vegan and Vegetarian Restaurant Model}
  - \fancyhead[R]{R-Bootcamp}
  - \renewcommand{\headrulewidth}{0pt}
  - \fancyfoot[C]{}
  - \fancyfoot[R]{\thepage}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, messages = FALSE)
packages <- c("dplyr","readxl", "curl", "ggplot2", "ggrepel", "maps", "plotly", "stringr", "tm", "wordcloud2", "tidyverse", "RColorBrewer", "ggwordcloud", "viridis", "bookdown", "utils")
package.check <- lapply(packages, FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
        install.packages(x, dependencies = TRUE)
        library(x, character.only = TRUE)
    }
})
```
\thispagestyle{empty}
\newpage
# Introduction

Inspired by Veganuary, we discovered a dataset about vegetarian and vegan restaurants in the US [@datafiniti2018]. The dataset was found through a dataset search on Google, hosted on Kaggel. The reason for selecting this dataset was that many useful parameters were already available. The description of the dataset states that it contains data from 18,0000 restaurants. Unfortunately, we only later noticed that only 211 individual restaurants with 10,000 menu items are included in the dataset, as it serves as an advertisement for the Datafiniti Business Database. The entire dataset is only available for purchase, and after consulting with the lecturer (Matteo Tanadini), it was agreed that the study should continue with the less meaningful results.

The purpose of this paper is to examine the relationship between the number of vegetarian or vegan restaurants, population and median household income for the U.S. zip code [@mpsc2020], and the frequency of Google searches for vegetarian and vegan restaurants in that state [@vegan_google_trends; @vegetarian_google_trends]. With the small sample size of 211 restaurants and 10000 menu items, this study cannot significantly predict the number of vegetarian restaurants in a zip code.
Nevertheless, this paper offers some insights into vegetarian and vegan supply in the U.S., as well as the demand and whether demographics, income, and Google searches match up.


# Methodology

In a first step relevant data has been researched. The data has been imported to R and first visualizations were made to get some insights into the data.  


**Model**
$$ \text{restaurants} = {\beta}_0 + {\beta}_1 * \text{residents} + {\beta}_2 * \text{income} + {\beta}_3 * \text{searches} $$

## Importing Data

We are gathering data from the following sources...

```{r importing, warning=FALSE}
#setwd("/Users/larissaeisele/switchdrive/R-Bootcamp/project-R_Bootcamp")
restaurants <-read.csv("./Data/veg_restaurants_US.csv")
income_pop_by_zip <- read_excel("./Data/MeanZIP-3.xlsx")
google_searches_vegetarian <- read_excel("./Data/searches_vegetarian_res.xlsx")
google_searches_vegan <- read_excel("./Data/searches_vegan_res.xlsx")
```

## Data Processing

### Restaurant Data
We are selecting the following data from the restaurant data set

```{r restaurant data, warning=FALSE}
restaurants <- restaurants %>% select(id, dateAdded, cuisines, latitude, longitude, menus.category, name, province, postalCode)
restaurants$postalCode <- as.numeric(restaurants$postalCode)
restaurants <- restaurants %>% mutate(cuisines_split = strsplit(cuisines,","))
```

Cleaning the missing data from on restaurant...
```{r restaurant missing data fix, warning=FALSE}
restaurants[restaurants$id == "AV1Tp59p3D1zeR_xE2DL",]$postalCode <- 94596
anyNA(restaurants$postalCode) # There are no missing values anymore in postalCode (zip code)
```


### Population Size and Mean Household Income Data

**Joining the Data**
```{r adding household mean and pop size to dataset, warning=FALSE}
joined_data <- left_join(restaurants, income_pop_by_zip %>% select(Zip, Mean, Pop), by = c("postalCode" = "Zip"))
```

**Checking for missing data**
```{r missing data check, warning=FALSE}
sum(is.na(joined_data$Mean)) # all postal codes have a mean household income
sum(is.na(joined_data$Pop)) # all postal codes have a population
```

### Google Search Data

```{r google search data, warning=FALSE}
# Vegetarian Restaurant Search Results
vegetarian_temp <- google_searches_vegetarian %>% group_by(province) %>% summarise(total_searches_vegetarian = sum(search_vegetarian_res, na.rm = TRUE))
joined_data <- left_join(joined_data, vegetarian_temp %>% select(province, total_searches_vegetarian), by = "province")
# Vegan Restaurant Search Results
vegan_temp <- google_searches_vegan %>% group_by(province) %>% summarise(total_searches_vegan = sum(search_vegan_res, na.rm = TRUE))
joined_data <- left_join(joined_data, vegan_temp %>% select(province, total_searches_vegan), by = "province")
```

# Data Visualization

## Histogram

In the following Histograms the amount of vegan and vegetarian restaurants are shown by state, to get some insights how the data is distributed by the region. 


```{r}
# Household Income by State

testing <- income_pop_by_zip$Mean
hist(testing, main = "Household Income by State",
     xlab = "Household Income")


```


## Boxplots

```{r}
#Boxplot Household Income
head(income_pop_by_zip)
#p <- ggplot(income_pop_by_zip, aes(x= Mean, y = Zip, fill = feed))
#p <- p + geom_boxplot()
#p <- p + theme_classic()
#p <- p + labs(title = "Mean Income by Zip Code")
#p
```


Checking for missing data

## Missing Values

```{r missing data check google search results, warning=FALSE}
sum(is.na(joined_data$total_searches_vegetarian))
sum(is.na(joined_data$total_searches_vegan))
# 42 provinces have no google search hits
```

\newpage
## Maps and Wordcloud

### Vegetarian and Vegan Restaurants in the U.S.
```{r, figures-side, fig.show="hold", out.width="50%", warning=FALSE, echo=TRUE}

par(mar = c(4, 4, .1, .1))

temp <- read.csv(curl("https://raw.githubusercontent.com/cphalpert/census-regions/master/us%20census%20bureau%20regions%20and%20divisions.csv"))
states_map <- map_data("state")
states_map <- left_join(states_map, temp %>% mutate(State = tolower(State)) %>% select(State, State.Code), by = c("region" = "State"))
google_search_frequency <- joined_data %>% group_by(province) %>% distinct(province, total_searches_vegan, total_searches_vegetarian) %>% summarise(google_search_frequency = total_searches_vegan + total_searches_vegetarian)

# Map Data
states_map_google_search_frequency <- left_join(states_map, google_search_frequency, by = c("State.Code" = "province"))
states_map_number_restaurants_state <- left_join(states_map, joined_data %>% group_by(province) %>% summarise(Number_Of_Restaurants = n_distinct(id)), by = c("State.Code" = "province"))
joined_data$MeanNormalized <- (joined_data$Mean - min(joined_data$Mean)) / (max(joined_data$Mean) - min(joined_data$Mean))

# Population
ggplot() +
  # Number of Restaurants by state
  geom_polygon(data = states_map_number_restaurants_state, aes(long, lat, group = group, fill=Number_Of_Restaurants), color = "white") + 
  scale_fill_gradientn("Number of Restaurants in State", colors = c('lightgreen', 'darkgreen'), breaks = seq(from = 0, to = 100, by = 15)) +
  
  # mapping 44 largest cities in the U.S. since 45 is in hawaii
  geom_point(data=us.cities %>% arrange(pop) %>% tail(44), aes(x=long, y=lat, size = pop/1000000), color = "blue", alpha = 0.4) +
  labs(size="Largest U.S. Cities Population") + scale_size(breaks=c(1, 3, 5, 8),labels=c("1 million","3 million","5 million","8 million"), guide="legend") +
  
  # mapping the veg. restaurants excluding hawaii (96753)
  geom_point(data=joined_data %>% filter(!str_detect(postalCode, "96753")), aes(x=longitude, y=latitude), size=0.3) +
  theme_void() + coord_map() + 
  theme(plot.title = element_text(face = "bold"), legend.position="bottom", legend.box="vertical", legend.margin=margin(), legend.box.just = "left") +
  labs(title = "Population")

# Income
ggplot() +
  # Number of Restaurants by state
  geom_polygon(data = states_map_number_restaurants_state, aes(long, lat, group = group, fill=Number_Of_Restaurants), color = "white") + 
  scale_fill_gradientn("Number of Restaurants in State         ", colors = c('lightgreen', 'darkgreen'), breaks = seq(from = 0, to = 100, by = 15)) +
  
  # mapping the veg. restaurants excluding hawaii (96753)
  geom_point(data=joined_data %>% filter(!str_detect(postalCode, "96753")), aes(x=longitude, y=latitude, color = MeanNormalized), size=0.5, alpha = 0.4) +
  scale_color_gradientn("Normalized Mean Household Income", colors = c('red', 'darkorange', 'yellow', 'green'), breaks = seq(from = 0, to = 1, by = 0.25)) +
  theme_void() + coord_map() + 
  theme(plot.title = element_text(face = "bold"), legend.position="bottom", legend.box="vertical", legend.margin=margin(), legend.box.just = "left") +
  labs(title = "Income") + 
  guides(fill = guide_colourbar(order = 1))

# Google Search Frequency
ggplot() +
  # Number of Google Searches by State
  geom_polygon(data = states_map_google_search_frequency, aes(long, lat, group = group, fill=google_search_frequency), color = "white") + 
  scale_fill_gradientn("Google Search Frequency in State", colors = c('lightblue', 'darkblue'), breaks = seq(from = 0, to = 5000, by = 1000)) +
  
  # mapping the veg. restaurants excluding hawaii (96753)
  geom_point(data=joined_data %>% filter(!str_detect(postalCode, "96753")), aes(x=longitude, y=latitude), size=0.5, alpha = 0.4, color = "black") +
  theme_void() + coord_map() + 
  theme(plot.title = element_text(face = "bold"), legend.position="bottom", legend.box="vertical", legend.margin=margin(0,125), legend.box.just = "left") +
  labs(title = "Google Search Frequency")

```



```{r, eval=knitr::is_html_output(), echo=FALSE}
knitr::asis_output("### Vegetarian and Vegan Cuisines")
```

```{r, eval=knitr::is_html_output(), warning=FALSE, fig.width=5, fig.height=5, echo=TRUE}
# devtools::install_github("lchiffon/wordcloud2") fixed rmarkdown
df <- as.data.frame(do.call(rbind, lapply(joined_data$cuisines_split, as.data.frame)))
colnames(df) <- c("cuisines")

# remove leading whitespaces
df$cuisines <- trimws(df$cuisines)

# additional fixes
df$cuisines[df$cuisines == 'Dim Sum'] <- 'DimSum'
df$cuisines[df$cuisines == 'Hot Dogs'] <- 'HotDogs'
df$cuisines[df$cuisines == 'Fast Food'] <- 'FastFood'
df$cuisines[df$cuisines == 'Deli Food'] <- 'Deli'
df$cuisines[df$cuisines == 'Ice Cream and Desserts'] <- 'IceCream and Desserts'

# remove nonsense words
df <- df %>% filter(!(str_detect(cuisines, "Restaurant|Friendly|Vegetarian|Vegan|Health|Bar|Gluten|Bistro|Take|Local|Organic|Steak|Cater|North|South|Middle")))

# seperate words by slash; the word ' and '; whitespace
filtered <- df %>% separate_rows(cuisines, sep = "/") %>% separate_rows(cuisines, sep = " and ") %>% separate_rows(cuisines, sep = " ")
filtered <- filtered %>% filter(!(str_detect(cuisines, "New|Modern|Small|Shops|Lunch|Pacific|American")))

# filtered$cuisines <- str_replace_all(filtered$cuisines, fixed(" "), "")

#filtered %>% distinct(cuisines)

v <- sort(rowSums(as.matrix(TermDocumentMatrix(Corpus((VectorSource(filtered$cuisines)))))),decreasing=TRUE)
word_frequency_df <- data.frame(word = names(v),freq=v)

wordcloud_colors <- colorRampPalette(brewer.pal(11,"BrBG"))
wordcloud2(word_frequency_df, size = 1, minRotation = pi/6, maxRotation = pi/6, rotateRatio = 1, color=rev(wordcloud_colors(32)[seq(1,32,1)]))
```
```{r, eval=knitr::is_latex_output(), echo=FALSE }
knitr::asis_output("### Vegetarian and Vegan Cuisines with ggplot since wordcloud2 can not be rendered into pdf...")
```

```{r, eval=knitr::is_latex_output(), warning=FALSE, fig.width=5, fig.height=5, echo=TRUE}
df <- as.data.frame(do.call(rbind, lapply(joined_data$cuisines_split, as.data.frame)))
colnames(df) <- c("cuisines")

# remove leading whitespaces
df$cuisines <- trimws(df$cuisines)

# additional fixes
df$cuisines[df$cuisines == 'Dim Sum'] <- 'DimSum'
df$cuisines[df$cuisines == 'Hot Dogs'] <- 'HotDogs'
df$cuisines[df$cuisines == 'Fast Food'] <- 'FastFood'
df$cuisines[df$cuisines == 'Deli Food'] <- 'Deli'
df$cuisines[df$cuisines == 'Ice Cream and Desserts'] <- 'IceCream and Desserts'

# remove nonsense words
df <- df %>% filter(!(str_detect(cuisines, "Restaurant|Friendly|Vegetarian|Vegan|Health|Bar|Gluten|Bistro|Take|Local|Organic|Steak|Cater|North|South|Middle")))

# seperate words by slash; the word ' and '; whitespace
filtered <- df %>% separate_rows(cuisines, sep = "/") %>% separate_rows(cuisines, sep = " and ") %>% separate_rows(cuisines, sep = " ")
filtered <- filtered %>% filter(!(str_detect(cuisines, "New|Modern|Small|Shops|Lunch|Pacific|American")))

# filtered$cuisines <- str_replace_all(filtered$cuisines, fixed(" "), "")

#filtered %>% distinct(cuisines)

v <- sort(rowSums(as.matrix(TermDocumentMatrix(Corpus((VectorSource(filtered$cuisines)))))),decreasing=TRUE)
word_frequency_df <- data.frame(word = names(v),freq=v)

ggplot(word_frequency_df, aes(label = word, size = freq, color = freq, angle = 30)) +
  geom_text_wordcloud(shape = "square", rm_outside = TRUE, max_steps = 1, grid_size = .1, eccentricity = .1) +
  scale_size_area(max_size = 10) +
  scale_color_viridis(option = "mako", direction = -1) + theme_void()
```


# Conclusions

\newpage
# References {-}
