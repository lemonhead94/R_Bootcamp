---
title: "Vegan & Vegetarian Restaurants"
author: "Jorit Studer and Larissa Eisele"
date: "2/1/2022"
output:
  html_document:
    df_print: paged
  bookdown::pdf_document2:
    pandoc_args:
    - --csl
    - apa.csl
  bookdown::html_document2:
    pandoc_args:
    - --csl
    - apa.csl
  pdf_document: default
bibliography: references.bib
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, messages = FALSE)
packages <- c("dplyr","readxl", "curl", "ggplot2", "ggrepel", "maps", "plotly", "stringr", "tm", "wordcloud2", "tidyverse", "RColorBrewer", "ggwordcloud", "viridis", "bookdown", "utils")
package.check <- lapply(packages, FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
        install.packages(x, dependencies = TRUE)
        library(x, character.only = TRUE)
    }
})
```

\newpage
# Introduction

The purpose of this paper is to examine the relationship between the number of vegetarian or vegan restaurants [@datafiniti2018], population and median household income for the U.S. zip code [@mpsc2020], and the frequency of Google searches for vegetarian and vegan restaurants in that state [@vegan_google_trends; @vegetarian_google_trends]. With a small sample size of 211 restaurants with 10000 menu items, this study cannot significantly predict the number of vegetarian restaurants in a zip code.

Nevertheless, this paper still gives some insights in regards to vegetarian and vegan offers in the U.S. as well as the demand from the population and if the offers and demand can be explained with demographic insight such as household income.


# Methodology

In a first step relevant data has been researched. The data has been imoported to R and first visualizations were made to get some insights into the data.  


**Model**
$$ \text{restaurants} = {\beta}_0 + {\beta}_1 * \text{residents} + {\beta}_2 * \text{income} + {\beta}_3 * \text{searches} $$

## Importing Data

We are gathering data from the following sources...

```{r importing, warning=FALSE}
#setwd("/Users/larissaeisele/switchdrive/R-Bootcamp/project-R_Bootcamp")
restaurants <-read.csv("./Data/veg_restaurants_US.csv")
income_pop_by_zip <- read_excel("./Data/MeanZIP-3.xlsx")
google_searches_vegetarian <- read_excel("./Data/searches_vegetarian_res.xlsx")
google_searches_vegan <- read_excel("./Data/searches_vegan_res.xlsx")
```

## Data Processing

### Restaurant Data
We are selecting the following data from the restaurant data set

```{r restaurant data, warning=FALSE}
restaurants <- restaurants %>% select(id, dateAdded, cuisines, latitude, longitude, menus.category, name, province, postalCode)
restaurants$postalCode <- as.numeric(restaurants$postalCode)
restaurants <- restaurants %>% mutate(cuisines_split = strsplit(cuisines,","))
```

Cleaning the missing data from on restaurant...
```{r restaurant missing data fix, warning=FALSE}
restaurants[restaurants$id == "AV1Tp59p3D1zeR_xE2DL",]$postalCode <- 94596
anyNA(restaurants$postalCode) # There are no missing values anymore in postalCode (zip code)
```


### Population Size and Mean Household Income Data

**Joining the Data**
```{r adding household mean and pop size to dataset, warning=FALSE}
joined_data <- left_join(restaurants, income_pop_by_zip %>% select(Zip, Mean, Pop), by = c("postalCode" = "Zip"))
```

**Checking for missing data**
```{r missing data check, warning=FALSE}
sum(is.na(joined_data$Mean)) # all postal codes have a mean household income
sum(is.na(joined_data$Pop)) # all postal codes have a population
```

### Google Search Data

```{r google search data, warning=FALSE}
# Vegetarian Restaurant Search Results
vegetarian_temp <- google_searches_vegetarian %>% group_by(province) %>% summarise(total_searches_vegetarian = sum(search_vegetarian_res, na.rm = TRUE))
joined_data <- left_join(joined_data, vegetarian_temp %>% select(province, total_searches_vegetarian), by = "province")
# Vegan Restaurant Search Results
vegan_temp <- google_searches_vegan %>% group_by(province) %>% summarise(total_searches_vegan = sum(search_vegan_res, na.rm = TRUE))
joined_data <- left_join(joined_data, vegan_temp %>% select(province, total_searches_vegan), by = "province")
```

# Data Visualization

## Histogram

In the following Histograms the amount of vegan and vegetarian restaurants are shown by state, to get some insights how the data is distributed by the region. 


```{r}
# Household Income by State

testing <- income_pop_by_zip$Mean
hist(testing, main = "Household Income by State",
     xlab = "Household Income")


```


## Boxplots

```{r}
#Boxplot Household Income
head(income_pop_by_zip)
p <- ggplot(income_pop_by_zip, aes(x= Mean, y = Zip, fill = feed))
p <- p + geom_boxplot()
p <- p + theme_classic()
p <- p + labs(title = "Mean Income by Zip Code")
p
```


Checking for missing data

## Missing Values

```{r missing data check google search results, warning=FALSE}
sum(is.na(joined_data$total_searches_vegetarian))
sum(is.na(joined_data$total_searches_vegan))
# 42 provinces have no google search hits
```

## Maps and Wordcloud

### Vegetarian and Vegan Restaurants in the U.S.
```{r, figures-side, fig.show="hold", out.width="50%", warning=FALSE, echo=FALSE}

par(mar = c(4, 4, .1, .1))

temp <- read.csv(curl("https://raw.githubusercontent.com/cphalpert/census-regions/master/us%20census%20bureau%20regions%20and%20divisions.csv"))
states_map <- map_data("state")
states_map <- left_join(states_map, temp %>% mutate(State = tolower(State)) %>% select(State, State.Code), by = c("region" = "State"))
google_search_frequency <- joined_data %>% group_by(province) %>% distinct(province, total_searches_vegan, total_searches_vegetarian) %>% summarise(google_search_frequency = total_searches_vegan + total_searches_vegetarian)

# Map Data
states_map_google_search_frequency <- left_join(states_map, google_search_frequency, by = c("State.Code" = "province"))
states_map_number_restaurants_state <- left_join(states_map, joined_data %>% group_by(province) %>% summarise(Number_Of_Restaurants = n_distinct(id)), by = c("State.Code" = "province"))
joined_data$MeanNormalized <- (joined_data$Mean - min(joined_data$Mean)) / (max(joined_data$Mean) - min(joined_data$Mean))

# Population
ggplot() +
  # Number of Restaurants by state
  geom_polygon(data = states_map_number_restaurants_state, aes(long, lat, group = group, fill=Number_Of_Restaurants), color = "white") + 
  scale_fill_gradientn("Number of Restaurants in State", colors = c('lightgreen', 'darkgreen'), breaks = seq(from = 0, to = 100, by = 15)) +
  
  # mapping 44 largest cities in the U.S. since 45 is in hawaii
  geom_point(data=us.cities %>% arrange(pop) %>% tail(44), aes(x=long, y=lat, size = pop/1000000), color = "blue", alpha = 0.4) +
  labs(size="Largest U.S. Cities Population") + scale_size(breaks=c(1, 3, 5, 8),labels=c("1 million","3 million","5 million","8 million"), guide="legend") +
  
  # mapping the veg. restaurants excluding hawaii (96753)
  geom_point(data=joined_data %>% filter(!str_detect(postalCode, "96753")), aes(x=longitude, y=latitude), size=0.3) +
  theme_void() + coord_map() + 
  theme(plot.title = element_text(face = "bold"), legend.position="bottom", legend.box="vertical", legend.margin=margin(), legend.box.just = "left") +
  labs(title = "Population")

# Income
ggplot() +
  # Number of Restaurants by state
  geom_polygon(data = states_map_number_restaurants_state, aes(long, lat, group = group, fill=Number_Of_Restaurants), color = "white") + 
  scale_fill_gradientn("Number of Restaurants in State         ", colors = c('lightgreen', 'darkgreen'), breaks = seq(from = 0, to = 100, by = 15)) +
  
  # mapping the veg. restaurants excluding hawaii (96753)
  geom_point(data=joined_data %>% filter(!str_detect(postalCode, "96753")), aes(x=longitude, y=latitude, color = MeanNormalized), size=0.5, alpha = 0.4) +
  scale_color_gradientn("Normalized Mean Household Income", colors = c('red', 'darkorange', 'yellow', 'green'), breaks = seq(from = 0, to = 1, by = 0.25)) +
  theme_void() + coord_map() + 
  theme(plot.title = element_text(face = "bold"), legend.position="bottom", legend.box="vertical", legend.margin=margin(), legend.box.just = "left") +
  labs(title = "Income") + 
  guides(fill = guide_colourbar(order = 1))

# Google Search Frequency
ggplot() +
  # Number of Google Searches by State
  geom_polygon(data = states_map_google_search_frequency, aes(long, lat, group = group, fill=google_search_frequency), color = "white") + 
  scale_fill_gradientn("Google Search Frequency in State", colors = c('lightblue', 'darkblue'), breaks = seq(from = 0, to = 5000, by = 1000)) +
  
  # mapping the veg. restaurants excluding hawaii (96753)
  geom_point(data=joined_data %>% filter(!str_detect(postalCode, "96753")), aes(x=longitude, y=latitude), size=0.5, alpha = 0.4, color = "black") +
  theme_void() + coord_map() + 
  theme(plot.title = element_text(face = "bold"), legend.position="bottom", legend.box="vertical", legend.margin=margin(0,125), legend.box.just = "left") +
  labs(title = "Google Search Frequency")

```



```{r, eval=knitr::is_html_output(), echo=FALSE}
knitr::asis_output("### Vegetarian and Vegan Cuisines")
```

```{r, eval=knitr::is_html_output(), warning=FALSE, fig.width=5, fig.height=5, echo=FALSE}
# devtools::install_github("lchiffon/wordcloud2") fixed rmarkdown
df <- as.data.frame(do.call(rbind, lapply(joined_data$cuisines_split, as.data.frame)))
colnames(df) <- c("cuisines")

# remove leading whitespaces
df$cuisines <- trimws(df$cuisines)

# additional fixes
df$cuisines[df$cuisines == 'Dim Sum'] <- 'DimSum'
df$cuisines[df$cuisines == 'Hot Dogs'] <- 'HotDogs'
df$cuisines[df$cuisines == 'Fast Food'] <- 'FastFood'
df$cuisines[df$cuisines == 'Deli Food'] <- 'Deli'
df$cuisines[df$cuisines == 'Ice Cream and Desserts'] <- 'IceCream and Desserts'

# remove nonsense words
df <- df %>% filter(!(str_detect(cuisines, "Restaurant|Friendly|Vegetarian|Vegan|Health|Bar|Gluten|Bistro|Take|Local|Organic|Steak|Cater|North|South|Middle")))

# seperate words by slash; the word ' and '; whitespace
filtered <- df %>% separate_rows(cuisines, sep = "/") %>% separate_rows(cuisines, sep = " and ") %>% separate_rows(cuisines, sep = " ")
filtered <- filtered %>% filter(!(str_detect(cuisines, "New|Modern|Small|Shops|Lunch|Pacific|American")))

# filtered$cuisines <- str_replace_all(filtered$cuisines, fixed(" "), "")

#filtered %>% distinct(cuisines)

v <- sort(rowSums(as.matrix(TermDocumentMatrix(Corpus((VectorSource(filtered$cuisines)))))),decreasing=TRUE)
word_frequency_df <- data.frame(word = names(v),freq=v)

wordcloud_colors <- colorRampPalette(brewer.pal(11,"BrBG"))
wordcloud2(word_frequency_df, size = 1, minRotation = pi/6, maxRotation = pi/6, rotateRatio = 1, color=rev(wordcloud_colors(32)[seq(1,32,1)]))
```
```{r, eval=knitr::is_latex_output(), echo=FALSE }
knitr::asis_output("### Vegetarian and Vegan Cuisines with ggplot since wordcloud2 can not be rendered into pdf...")
```

```{r, eval=knitr::is_latex_output(), warning=FALSE, fig.width=5, fig.height=5, echo=FALSE}
df <- as.data.frame(do.call(rbind, lapply(joined_data$cuisines_split, as.data.frame)))
colnames(df) <- c("cuisines")

# remove leading whitespaces
df$cuisines <- trimws(df$cuisines)

# additional fixes
df$cuisines[df$cuisines == 'Dim Sum'] <- 'DimSum'
df$cuisines[df$cuisines == 'Hot Dogs'] <- 'HotDogs'
df$cuisines[df$cuisines == 'Fast Food'] <- 'FastFood'
df$cuisines[df$cuisines == 'Deli Food'] <- 'Deli'
df$cuisines[df$cuisines == 'Ice Cream and Desserts'] <- 'IceCream and Desserts'

# remove nonsense words
df <- df %>% filter(!(str_detect(cuisines, "Restaurant|Friendly|Vegetarian|Vegan|Health|Bar|Gluten|Bistro|Take|Local|Organic|Steak|Cater|North|South|Middle")))

# seperate words by slash; the word ' and '; whitespace
filtered <- df %>% separate_rows(cuisines, sep = "/") %>% separate_rows(cuisines, sep = " and ") %>% separate_rows(cuisines, sep = " ")
filtered <- filtered %>% filter(!(str_detect(cuisines, "New|Modern|Small|Shops|Lunch|Pacific|American")))

# filtered$cuisines <- str_replace_all(filtered$cuisines, fixed(" "), "")

#filtered %>% distinct(cuisines)

v <- sort(rowSums(as.matrix(TermDocumentMatrix(Corpus((VectorSource(filtered$cuisines)))))),decreasing=TRUE)
word_frequency_df <- data.frame(word = names(v),freq=v)

ggplot(word_frequency_df, aes(label = word, size = freq, color = freq, angle = 30)) +
  geom_text_wordcloud(shape = "square", rm_outside = TRUE, max_steps = 1, grid_size = .1, eccentricity = .1) +
  scale_size_area(max_size = 10) +
  scale_color_viridis(option = "mako", direction = -1) + theme_void()
```


# Testing

\newpage
# References {-}
